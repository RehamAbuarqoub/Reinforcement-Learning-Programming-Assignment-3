{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def22c67",
   "metadata": {},
   "source": [
    "# CSCN8020 â€“ Assignment 3: Deep Q-Learning on Pong\n",
    "\n",
    "This notebook implements a Deep Q-Network (DQN) agent to play **PongDeterministic** using **Gymnasium + ALE** and **PyTorch**. The code is written to work with Python 3.11 in a virtual environment (`pong311`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e8e528",
   "metadata": {},
   "source": [
    "###  Imports and Global Configuration\n",
    "This cell loads all required Python libraries for the DQN implementation, including:\n",
    "- `gym` for the Pong environment  \n",
    "- `numpy` and `PIL` for preprocessing  \n",
    "- `torch` for building and training the Deep Q-Network  \n",
    "- plotting tools for visualization  \n",
    "\n",
    "It also sets global configurations such as the plotting style and device selection (CPU/GPU).  \n",
    "A small NumPy compatibility patch is included to avoid boolean type conflicts with Gym.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b66ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Gym version: 0.26.2\n",
      "NumPy version: 2.3.4\n",
      "PyTorch version: 2.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Imports and global configuration\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ðŸ”§ Compatibility patch for NumPy 2.x + Gym 0.26\n",
    "if not hasattr(np, \"bool8\"):\n",
    "    np.bool8 = np.bool_\n",
    "\n",
    "import gym\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Configure plots\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "\n",
    "# Select device: use GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "print(\"Gym version:\", gym.__version__)\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c96656f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Gym version: 0.26.2\n",
      "NumPy version: 2.3.4\n",
      "PyTorch version: 2.9.1+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Imports and global configuration\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import gym          # ðŸ‘ˆ ONLY this, no 'gymnasium' import\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Configure plots\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "\n",
    "# Select device: use GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "print(\"Gym version:\", gym.__version__)\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce8823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ID = \"ALE/PongDeterministic-v5\"\n",
    "# or ENV_ID = \"ALE/Pong-v5\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92457f28",
   "metadata": {},
   "source": [
    "### 2. Environment Setup\n",
    "This cell creates the PongDeterministic-v4 environment and performs a quick sanity check.\n",
    "It displays:\n",
    "- the observation shape (210Ã—160Ã—3 raw frames)\n",
    "- the action space (Discrete(6))\n",
    "- and verifies that the environment resets correctly.\n",
    "\n",
    "This step ensures the Atari ROMs and Gym installation are functioning properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc95198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw observation shape: (210, 160, 3)\n",
      "Action space: Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Environment definition\n",
    "# =====================\n",
    "\n",
    "ENV_ID = \"PongDeterministic-v4\"   # OpenAI Gym Atari env\n",
    "\n",
    "def make_env(render_mode: str | None = None):\n",
    "    \"\"\"Create a PongDeterministic environment.\"\"\"\n",
    "    env = gym.make(ENV_ID)   # keep it simple, ignore render_mode for now\n",
    "    return env\n",
    "\n",
    "# Quick sanity check\n",
    "env = make_env()\n",
    "obs, info = env.reset()\n",
    "print(\"Raw observation shape:\", obs.shape)\n",
    "print(\"Action space:\", env.action_space)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7fa149",
   "metadata": {},
   "source": [
    "### 3. Frame Preprocessing Functions\n",
    "This section defines the helper functions required to transform raw Atari frames:\n",
    "- cropping the game area  \n",
    "- converting to grayscale  \n",
    "- resizing to 84Ã—84  \n",
    "- normalizing pixel values  \n",
    "- stacking consecutive frames  \n",
    "\n",
    "These transformations are standard in DQN implementations and allow the CNN to learn a clean and stable representation of game states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a453a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed frame shape (single): (84, 84)\n",
      "Stacked state shape: (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Preprocessing utilities\n",
    "# =====================\n",
    "\n",
    "def crop_pong(img: np.ndarray, top: int = 34, bottom: int = 194) -> np.ndarray:\n",
    "    \"\"\"Crop raw (210,160,3) RGB frame to remove score/borders.\"\"\"\n",
    "    assert img.ndim == 3 and img.shape[2] == 3, \"Expect RGB array of shape (H,W,3)\"\n",
    "    return img[top:bottom, :, :]\n",
    "\n",
    "\n",
    "def to_grayscale(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"RGB -> grayscale using luminosity method. Returns (H,W) float32 in [0,255].\"\"\"\n",
    "    r = img[..., 0].astype(np.float32)\n",
    "    g = img[..., 1].astype(np.float32)\n",
    "    b = img[..., 2].astype(np.float32)\n",
    "    gray = 0.299 * r + 0.587 * g + 0.114 * b\n",
    "    return gray\n",
    "\n",
    "\n",
    "def resize_img(img: np.ndarray, out_h: int = 84, out_w: int = 84) -> np.ndarray:\n",
    "    \"\"\"Resize grayscale image using PIL BILINEAR to (out_h, out_w).\"\"\"\n",
    "    # If 2D, add a dummy channel so PIL is happy\n",
    "    if img.ndim == 2:\n",
    "        pil_img = Image.fromarray(img.astype(np.uint8))\n",
    "    else:\n",
    "        pil_img = Image.fromarray(img.astype(np.uint8))\n",
    "    pil_img = pil_img.resize((out_w, out_h), Image.BILINEAR)\n",
    "    return np.array(pil_img, dtype=np.uint8)\n",
    "\n",
    "\n",
    "def preprocess_frame(frame: np.ndarray, crop: bool = True) -> np.ndarray:\n",
    "    \"\"\"Full preprocessing pipeline for a single Pong frame.\n",
    "\n",
    "\n",
    "    Steps:\n",
    "\n",
    "    1. Optionally crop to game area.\n",
    "\n",
    "    2. Convert to grayscale.\n",
    "\n",
    "    3. Resize to 84x84.\n",
    "\n",
    "    4. Normalize to [0,1].\n",
    "\n",
    "    \"\"\"\n",
    "    if crop:\n",
    "        frame = crop_pong(frame)\n",
    "    gray = to_grayscale(frame)\n",
    "    resized = resize_img(gray, 84, 84)\n",
    "    normalized = resized / 255.0\n",
    "    return normalized.astype(np.float32)\n",
    "\n",
    "\n",
    "def stack_frames(state_deque: deque | None,\n",
    "                 new_frame: np.ndarray,\n",
    "                 stack_size: int) -> tuple[np.ndarray, deque]:\n",
    "    \"\"\"Maintain a deque of the last `stack_size` preprocessed frames.\n",
    "\n",
    "\n",
    "    Returns the stacked state of shape (stack_size, 84, 84) and the updated deque.\n",
    "\n",
    "    \"\"\"\n",
    "    processed = preprocess_frame(new_frame)\n",
    "    if state_deque is None:\n",
    "        state_deque = deque([processed] * stack_size, maxlen=stack_size)\n",
    "    else:\n",
    "        state_deque.append(processed)\n",
    "\n",
    "    stacked_state = np.stack(state_deque, axis=0)  # (C, H, W)\n",
    "    return stacked_state, state_deque\n",
    "\n",
    "\n",
    "# Quick visual sanity check\n",
    "env = make_env()\n",
    "frame, info = env.reset()\n",
    "proc = preprocess_frame(frame)\n",
    "print(\"Processed frame shape (single):\", proc.shape)\n",
    "\n",
    "stack_size = 4\n",
    "state_deque = None\n",
    "stacked_state, state_deque = stack_frames(state_deque, frame, stack_size)\n",
    "print(\"Stacked state shape:\", stacked_state.shape)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302cc86b",
   "metadata": {},
   "source": [
    "### 4. Replay Memory Buffer\n",
    "This cell implements the ReplayBuffer class used to store past transitions.\n",
    "\n",
    "Replay memory allows the agent to:\n",
    "- break correlations between sequential frames  \n",
    "- train using batches of random experiences  \n",
    "- stabilize the Q-learning updates  \n",
    "\n",
    "This is a core requirement for Deep Q-Learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d54b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty replay buffer length: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Experience replay (replay buffer)\n",
    "# =============================\n",
    "\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"reward\", \"next_state\", \"done\"))\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self,\n",
    "             state: np.ndarray,\n",
    "             action: int,\n",
    "             reward: float,\n",
    "             next_state: np.ndarray,\n",
    "             done: float) -> None:\n",
    "        \"\"\"Save a transition to the replay buffer.\"\"\"\n",
    "        self.memory.append(Transition(state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int) -> Transition:\n",
    "        \"\"\"Randomly sample a batch of transitions.\"\"\"\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        return Transition(*zip(*batch))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "# Quick size check\n",
    "memory = ReplayMemory(10_000)\n",
    "print(\"Empty replay buffer length:\", len(memory))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05f9d9",
   "metadata": {},
   "source": [
    "### 5. Deep Q-Network (DQN) Architecture\n",
    "This cell defines the CNN model that approximates the Q-function.\n",
    "\n",
    "The architecture includes:\n",
    "- three convolutional layers  \n",
    "- two fully connected layers  \n",
    "- an output layer representing Q-values for the 6 Pong actions  \n",
    "\n",
    "This design follows the original DeepMind DQN paper used for Atari.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76912ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed state shape (C,H,W): (4, 84, 84)\n",
      "DQN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Deep Q-Network definition\n",
    "# =========================\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape: Tuple[int, int, int], num_actions: int):\n",
    "        super().__init__()\n",
    "        c, h, w = input_shape\n",
    "\n",
    "        # Convolutional feature extractor (similar to Nature DQN)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Compute the size of the linear layer by passing a dummy tensor\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, c, h, w)\n",
    "            n_flatten = self.features(dummy).view(1, -1).shape[1]\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_flatten, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Instantiate networks and verify shapes\n",
    "env = make_env()\n",
    "obs, info = env.reset()\n",
    "num_actions = env.action_space.n\n",
    "stack_size = 4\n",
    "\n",
    "state_deque = None\n",
    "state, state_deque = stack_frames(state_deque, obs, stack_size)\n",
    "print(\"Processed state shape (C,H,W):\", state.shape)\n",
    "\n",
    "policy_net = DQN(state.shape, num_actions).to(device)\n",
    "target_net = DQN(state.shape, num_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "print(policy_net)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f93fab9",
   "metadata": {},
   "source": [
    "### 6. Action Selection Strategy (Epsilon-Greedy)\n",
    "This cell defines how the agent chooses actions.\n",
    "\n",
    "- With probability Îµ, it explores (random action).  \n",
    "- With probability 1âˆ’Îµ, it exploits the DQNâ€™s predicted Q-values.  \n",
    "- Îµ decays over time to shift from exploration to exploitation.  \n",
    "\n",
    "This balances learning new behaviors and improving known strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cf9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Hyperparameters & policy\n",
    "# =======================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# ------------ DEBUG MODE ------------\n",
    "# Use DEBUG_MODE = True while testing / debugging\n",
    "# Use DEBUG_MODE = False for final runs used in your report\n",
    "DEBUG_MODE = True\n",
    "\n",
    "# ------------ Assignment-required hyperparameters ------------\n",
    "# Mini-batch size: must be 8 (default) or 16 (experiment)\n",
    "BATCH_SIZE = 8   # change manually to 16 for the second experiment\n",
    "\n",
    "GAMMA = 0.95\n",
    "\n",
    "# Epsilon-greedy exploration (assignment formula)\n",
    "EPS_INIT = 1.0\n",
    "EPS_DECAY = 0.995\n",
    "EPS_MIN = 0.05\n",
    "\n",
    "# Target network update rate in EPISODES (not frames)\n",
    "# Default: 10 episodes; experiment: 3 episodes\n",
    "TARGET_UPDATE_EPISODES = 10   # change manually to 3 for the target-update experiment\n",
    "\n",
    "# Replay buffer and optimizer\n",
    "REPLAY_SIZE = 100_000\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Training length limits (you are allowed to choose these)\n",
    "if DEBUG_MODE:\n",
    "    MAX_FRAMES = 20_000      # small for faster debug runs\n",
    "    MAX_EPISODES = 150\n",
    "else:\n",
    "    MAX_FRAMES = 150_000     # reasonable for final runs (can adjust)\n",
    "    MAX_EPISODES = 500\n",
    "\n",
    "# Start training only after we have some transitions\n",
    "MIN_REPLAY_SIZE_FOR_TRAINING = 1_000\n",
    "\n",
    "# Early stopping (stopper) settings\n",
    "MIN_FRAMES_BEFORE_STOP = 60_000   # only check stopper after this many frames\n",
    "EARLY_STOP_WINDOW = 5             # average reward over last 5 episodes\n",
    "EARLY_STOP_TARGET = 15.0          # Pong \"good\" performance threshold\n",
    "\n",
    "# Create replay memory and optimizer\n",
    "memory = ReplayMemory(REPLAY_SIZE)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Global epsilon value (updated every step)\n",
    "epsilon = EPS_INIT\n",
    "\n",
    "\n",
    "def update_epsilon(eps: float) -> float:\n",
    "    \"\"\"\n",
    "    Update epsilon according to the assignment rule:\n",
    "\n",
    "        eps = eps * EPS_DECAY if eps >= EPS_MIN\n",
    "            = EPS_MIN otherwise\n",
    "    \"\"\"\n",
    "    if eps >= EPS_MIN:\n",
    "        eps = eps * EPS_DECAY\n",
    "    else:\n",
    "        eps = EPS_MIN\n",
    "    return eps\n",
    "\n",
    "\n",
    "def select_action(state: np.ndarray, epsilon: float) -> int:\n",
    "    \"\"\"Epsilon-greedy action selection on the current policy network.\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(num_actions)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_v = torch.tensor(\n",
    "                state,\n",
    "                dtype=torch.float32,\n",
    "                device=device\n",
    "            ).unsqueeze(0)\n",
    "            q_values = policy_net(state_v)\n",
    "            action = int(q_values.argmax(dim=1).item())\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a4f8f7",
   "metadata": {},
   "source": [
    "### 7. Optimization Step\n",
    "This cell implements the function that performs a single gradient update using:\n",
    "- sampled transitions from the replay buffer  \n",
    "- the Bellman target  \n",
    "- Huber loss  \n",
    "- target network stabilization  \n",
    "\n",
    "This is where actual learning happens in the DQN algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4623f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================\n",
    "# Optimization (SGD)\n",
    "# ==================\n",
    "\n",
    "def optimize_model() -> float:\n",
    "    \"\"\"\n",
    "    Sample a batch from replay memory and perform one gradient step.\n",
    "\n",
    "    Returns the scalar loss (float) for logging,\n",
    "    or 0.0 if no update was performed.\n",
    "    \"\"\"\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return 0.0\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*transitions)\n",
    "\n",
    "    # Convert batch-arrays to tensors\n",
    "    state_batch = torch.tensor(np.array(batch.state), dtype=torch.float32, device=device)\n",
    "    action_batch = torch.tensor(batch.action, dtype=torch.int64, device=device).unsqueeze(1)\n",
    "    reward_batch = torch.tensor(batch.reward, dtype=torch.float32, device=device)\n",
    "    next_state_batch = torch.tensor(np.array(batch.next_state), dtype=torch.float32, device=device)\n",
    "    done_batch = torch.tensor(batch.done, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Q(s,a) for the actions actually taken\n",
    "    q_values = policy_net(state_batch).gather(1, action_batch).squeeze(1)\n",
    "\n",
    "    # Max_a' Q_target(s', a')\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_net(next_state_batch).max(1)[0]\n",
    "        target = reward_batch + GAMMA * next_q_values * (1.0 - done_batch)\n",
    "\n",
    "    # Huber loss\n",
    "    loss = F.smooth_l1_loss(q_values, target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Clip gradients to avoid exploding gradients\n",
    "    nn.utils.clip_grad_norm_(policy_net.parameters(), 10.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd5e1f",
   "metadata": {},
   "source": [
    "### 8. Training Setup and Hyperparameters\n",
    "This cell defines all key training hyperparameters:\n",
    "- learning rate  \n",
    "- gamma (discount factor)  \n",
    "- replay memory size  \n",
    "- target network update frequency  \n",
    "- epsilon decay  \n",
    "- maximum training frames  \n",
    "\n",
    "These settings control the learning behavior and performance of the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b567f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    1 | Frame    1038 | Reward  -20.0 | Epsilon 0.997 | Replay    1038 | Elapsed   0.0 min\n",
      "Episode    2 | Frame    1876 | Reward  -20.0 | Epsilon 0.994 | Replay    1876 | Elapsed   0.1 min\n",
      "Episode    3 | Frame    2640 | Reward  -21.0 | Epsilon 0.992 | Replay    2640 | Elapsed   0.1 min\n",
      "Episode    4 | Frame    3544 | Reward  -21.0 | Epsilon 0.989 | Replay    3544 | Elapsed   0.1 min\n",
      "Episode    5 | Frame    4524 | Reward  -19.0 | Epsilon 0.986 | Replay    4524 | Elapsed   0.1 min\n",
      "Episode    6 | Frame    5706 | Reward  -19.0 | Epsilon 0.983 | Replay    5706 | Elapsed   0.2 min\n",
      "Episode    7 | Frame    6653 | Reward  -20.0 | Epsilon 0.980 | Replay    6653 | Elapsed   0.2 min\n",
      "Episode    8 | Frame    7585 | Reward  -21.0 | Epsilon 0.977 | Replay    7585 | Elapsed   0.3 min\n",
      "Episode    9 | Frame    8484 | Reward  -21.0 | Epsilon 0.975 | Replay    8484 | Elapsed   0.3 min\n",
      "Episode   10 | Frame    9336 | Reward  -21.0 | Epsilon 0.972 | Replay    9336 | Elapsed   0.3 min\n",
      "Episode   11 | Frame   10130 | Reward  -21.0 | Epsilon 0.970 | Replay   10130 | Elapsed   0.3 min\n",
      "Episode   12 | Frame   11120 | Reward  -20.0 | Epsilon 0.967 | Replay   11120 | Elapsed   0.4 min\n",
      "Episode   13 | Frame   11944 | Reward  -21.0 | Epsilon 0.964 | Replay   11944 | Elapsed   0.4 min\n",
      "Episode   14 | Frame   12861 | Reward  -20.0 | Epsilon 0.961 | Replay   12861 | Elapsed   0.5 min\n",
      "Episode   15 | Frame   13765 | Reward  -20.0 | Epsilon 0.959 | Replay   13765 | Elapsed   0.5 min\n",
      "Episode   16 | Frame   14818 | Reward  -19.0 | Epsilon 0.956 | Replay   14818 | Elapsed   0.5 min\n",
      "Episode   17 | Frame   15689 | Reward  -20.0 | Epsilon 0.953 | Replay   15689 | Elapsed   0.6 min\n",
      "Episode   18 | Frame   16621 | Reward  -20.0 | Epsilon 0.950 | Replay   16621 | Elapsed   0.6 min\n",
      "Episode   19 | Frame   17612 | Reward  -21.0 | Epsilon 0.947 | Replay   17612 | Elapsed   0.6 min\n",
      "Episode   20 | Frame   18436 | Reward  -21.0 | Epsilon 0.945 | Replay   18436 | Elapsed   0.7 min\n",
      "Episode   21 | Frame   19373 | Reward  -19.0 | Epsilon 0.942 | Replay   19373 | Elapsed   0.7 min\n",
      "Episode   22 | Frame   20226 | Reward  -21.0 | Epsilon 0.939 | Replay   20226 | Elapsed   0.8 min\n",
      "Episode   23 | Frame   21139 | Reward  -21.0 | Epsilon 0.937 | Replay   21139 | Elapsed   0.8 min\n",
      "Episode   24 | Frame   22083 | Reward  -21.0 | Epsilon 0.934 | Replay   22083 | Elapsed   0.8 min\n",
      "Episode   25 | Frame   23046 | Reward  -20.0 | Epsilon 0.931 | Replay   23046 | Elapsed   0.9 min\n",
      "Episode   26 | Frame   23898 | Reward  -21.0 | Epsilon 0.928 | Replay   23898 | Elapsed   0.9 min\n",
      "Episode   27 | Frame   24864 | Reward  -20.0 | Epsilon 0.925 | Replay   24864 | Elapsed   1.0 min\n",
      "Episode   28 | Frame   25628 | Reward  -21.0 | Epsilon 0.923 | Replay   25628 | Elapsed   1.0 min\n",
      "Episode   29 | Frame   26593 | Reward  -20.0 | Epsilon 0.920 | Replay   26593 | Elapsed   1.0 min\n",
      "Episode   30 | Frame   27580 | Reward  -20.0 | Epsilon 0.917 | Replay   27580 | Elapsed   1.1 min\n",
      "Episode   31 | Frame   28433 | Reward  -21.0 | Epsilon 0.915 | Replay   28433 | Elapsed   1.1 min\n",
      "Episode   32 | Frame   29317 | Reward  -21.0 | Epsilon 0.912 | Replay   29317 | Elapsed   1.2 min\n",
      "Episode   33 | Frame   30341 | Reward  -20.0 | Epsilon 0.909 | Replay   30341 | Elapsed   1.2 min\n",
      "Episode   34 | Frame   31165 | Reward  -21.0 | Epsilon 0.907 | Replay   31165 | Elapsed   1.3 min\n",
      "Episode   35 | Frame   32208 | Reward  -20.0 | Epsilon 0.903 | Replay   32208 | Elapsed   1.3 min\n",
      "Episode   36 | Frame   33416 | Reward  -19.0 | Epsilon 0.900 | Replay   33416 | Elapsed   1.4 min\n",
      "Episode   37 | Frame   34240 | Reward  -21.0 | Epsilon 0.897 | Replay   34240 | Elapsed   1.4 min\n",
      "Episode   38 | Frame   35111 | Reward  -20.0 | Epsilon 0.895 | Replay   35111 | Elapsed   1.5 min\n",
      "Episode   39 | Frame   35924 | Reward  -21.0 | Epsilon 0.892 | Replay   35924 | Elapsed   1.5 min\n",
      "Episode   40 | Frame   36839 | Reward  -21.0 | Epsilon 0.889 | Replay   36839 | Elapsed   1.5 min\n",
      "Episode   41 | Frame   37898 | Reward  -19.0 | Epsilon 0.886 | Replay   37898 | Elapsed   1.6 min\n",
      "Episode   42 | Frame   38722 | Reward  -21.0 | Epsilon 0.884 | Replay   38722 | Elapsed   1.6 min\n",
      "Episode   43 | Frame   39704 | Reward  -20.0 | Epsilon 0.881 | Replay   39704 | Elapsed   1.7 min\n",
      "Episode   44 | Frame   40623 | Reward  -20.0 | Epsilon 0.878 | Replay   40623 | Elapsed   1.7 min\n",
      "Episode   45 | Frame   41608 | Reward  -19.0 | Epsilon 0.875 | Replay   41608 | Elapsed   1.8 min\n",
      "Episode   46 | Frame   42372 | Reward  -21.0 | Epsilon 0.873 | Replay   42372 | Elapsed   1.8 min\n",
      "Episode   47 | Frame   43352 | Reward  -20.0 | Epsilon 0.870 | Replay   43352 | Elapsed   1.9 min\n",
      "Episode   48 | Frame   44204 | Reward  -21.0 | Epsilon 0.867 | Replay   44204 | Elapsed   1.9 min\n",
      "Episode   49 | Frame   45184 | Reward  -19.0 | Epsilon 0.864 | Replay   45184 | Elapsed   2.0 min\n",
      "Episode   50 | Frame   46062 | Reward  -21.0 | Epsilon 0.862 | Replay   46062 | Elapsed   2.0 min\n",
      "Episode   51 | Frame   47045 | Reward  -20.0 | Epsilon 0.859 | Replay   47045 | Elapsed   2.1 min\n",
      "Episode   52 | Frame   47869 | Reward  -21.0 | Epsilon 0.856 | Replay   47869 | Elapsed   2.1 min\n",
      "Episode   53 | Frame   48693 | Reward  -21.0 | Epsilon 0.854 | Replay   48693 | Elapsed   2.2 min\n",
      "Episode   54 | Frame   49485 | Reward  -21.0 | Epsilon 0.852 | Replay   49485 | Elapsed   2.2 min\n",
      "Episode   55 | Frame   50458 | Reward  -21.0 | Epsilon 0.849 | Replay   50458 | Elapsed   3.2 min\n",
      "Episode   56 | Frame   51341 | Reward  -21.0 | Epsilon 0.846 | Replay   51341 | Elapsed   5.1 min\n",
      "Episode   57 | Frame   52105 | Reward  -21.0 | Epsilon 0.844 | Replay   52105 | Elapsed   6.9 min\n",
      "Episode   58 | Frame   52976 | Reward  -20.0 | Epsilon 0.841 | Replay   52976 | Elapsed   8.9 min\n",
      "Episode   59 | Frame   53800 | Reward  -21.0 | Epsilon 0.839 | Replay   53800 | Elapsed  10.9 min\n",
      "Episode   60 | Frame   54691 | Reward  -20.0 | Epsilon 0.836 | Replay   54691 | Elapsed  13.1 min\n",
      "Episode   61 | Frame   55593 | Reward  -20.0 | Epsilon 0.833 | Replay   55593 | Elapsed  15.3 min\n",
      "Episode   62 | Frame   56575 | Reward  -20.0 | Epsilon 0.830 | Replay   56575 | Elapsed  17.7 min\n",
      "Episode   63 | Frame   57490 | Reward  -20.0 | Epsilon 0.828 | Replay   57490 | Elapsed  20.1 min\n",
      "Episode   64 | Frame   58301 | Reward  -21.0 | Epsilon 0.825 | Replay   58301 | Elapsed  22.2 min\n",
      "Episode   65 | Frame   59065 | Reward  -21.0 | Epsilon 0.823 | Replay   59065 | Elapsed  24.2 min\n",
      "Episode   66 | Frame   59829 | Reward  -21.0 | Epsilon 0.821 | Replay   59829 | Elapsed  26.1 min\n",
      "Episode   67 | Frame   60711 | Reward  -21.0 | Epsilon 0.818 | Replay   60711 | Elapsed  28.3 min\n",
      "Episode   68 | Frame   61549 | Reward  -20.0 | Epsilon 0.815 | Replay   61549 | Elapsed  30.4 min\n",
      "Episode   69 | Frame   62493 | Reward  -20.0 | Epsilon 0.813 | Replay   62493 | Elapsed  32.9 min\n",
      "Episode   70 | Frame   63500 | Reward  -20.0 | Epsilon 0.810 | Replay   63500 | Elapsed  35.5 min\n",
      "Episode   71 | Frame   64373 | Reward  -21.0 | Epsilon 0.807 | Replay   64373 | Elapsed  37.8 min\n",
      "Episode   72 | Frame   65245 | Reward  -21.0 | Epsilon 0.804 | Replay   65245 | Elapsed  40.1 min\n",
      "Episode   73 | Frame   66226 | Reward  -20.0 | Epsilon 0.801 | Replay   66226 | Elapsed  42.5 min\n",
      "Episode   74 | Frame   67158 | Reward  -19.0 | Epsilon 0.799 | Replay   67158 | Elapsed  45.0 min\n",
      "Episode   75 | Frame   67983 | Reward  -21.0 | Epsilon 0.796 | Replay   67983 | Elapsed  47.0 min\n",
      "Episode   76 | Frame   68909 | Reward  -20.0 | Epsilon 0.793 | Replay   68909 | Elapsed  49.3 min\n",
      "Episode   77 | Frame   69733 | Reward  -21.0 | Epsilon 0.791 | Replay   69733 | Elapsed  51.4 min\n",
      "Episode   78 | Frame   70559 | Reward  -21.0 | Epsilon 0.788 | Replay   70559 | Elapsed  53.5 min\n",
      "Episode   79 | Frame   71501 | Reward  -21.0 | Epsilon 0.785 | Replay   71501 | Elapsed  56.1 min\n",
      "Episode   80 | Frame   72293 | Reward  -21.0 | Epsilon 0.783 | Replay   72293 | Elapsed  58.2 min\n",
      "Episode   81 | Frame   73237 | Reward  -21.0 | Epsilon 0.780 | Replay   73237 | Elapsed  60.6 min\n",
      "Episode   82 | Frame   74123 | Reward  -20.0 | Epsilon 0.778 | Replay   74123 | Elapsed  62.9 min\n",
      "Episode   83 | Frame   75142 | Reward  -20.0 | Epsilon 0.775 | Replay   75142 | Elapsed  65.5 min\n",
      "Episode   84 | Frame   75906 | Reward  -21.0 | Epsilon 0.772 | Replay   75906 | Elapsed  67.4 min\n",
      "Episode   85 | Frame   76804 | Reward  -20.0 | Epsilon 0.770 | Replay   76804 | Elapsed  69.7 min\n",
      "Episode   86 | Frame   77750 | Reward  -21.0 | Epsilon 0.767 | Replay   77750 | Elapsed  72.3 min\n",
      "Episode   87 | Frame   78606 | Reward  -20.0 | Epsilon 0.764 | Replay   78606 | Elapsed  74.7 min\n",
      "Episode   88 | Frame   79740 | Reward  -19.0 | Epsilon 0.761 | Replay   79740 | Elapsed  77.9 min\n",
      "Episode   89 | Frame   80564 | Reward  -21.0 | Epsilon 0.758 | Replay   80564 | Elapsed  80.1 min\n",
      "Episode   90 | Frame   81495 | Reward  -20.0 | Epsilon 0.756 | Replay   81495 | Elapsed  82.5 min\n",
      "Episode   91 | Frame   82447 | Reward  -19.0 | Epsilon 0.753 | Replay   82447 | Elapsed  85.0 min\n",
      "Episode   92 | Frame   83394 | Reward  -20.0 | Epsilon 0.750 | Replay   83394 | Elapsed  87.4 min\n",
      "Episode   93 | Frame   84251 | Reward  -21.0 | Epsilon 0.747 | Replay   84251 | Elapsed  89.5 min\n",
      "Episode   94 | Frame   85094 | Reward  -21.0 | Epsilon 0.745 | Replay   85094 | Elapsed  91.8 min\n",
      "Episode   95 | Frame   86209 | Reward  -18.0 | Epsilon 0.741 | Replay   86209 | Elapsed  94.9 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Start learning only after the replay buffer has some data\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m frame_idx > INITIAL_EXPLORATION_FRAMES:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     loss = \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m loss != \u001b[32m0.0\u001b[39m:\n\u001b[32m     46\u001b[39m         losses.append(loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36moptimize_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Max_a' Q_target(s', a')\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     next_q_values = \u001b[43mtarget_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state_batch\u001b[49m\u001b[43m)\u001b[49m.max(\u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m     31\u001b[39m     target = reward_batch + GAMMA * next_q_values * (\u001b[32m1.0\u001b[39m - done_batch)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Huber loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RAJEH\\Documents\\GitHub\\Reinforcement-Learning-Programming-Assignment-3\\pong311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RAJEH\\Documents\\GitHub\\Reinforcement-Learning-Programming-Assignment-3\\pong311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mDQN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     x = x.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RAJEH\\Documents\\GitHub\\Reinforcement-Learning-Programming-Assignment-3\\pong311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RAJEH\\Documents\\GitHub\\Reinforcement-Learning-Programming-Assignment-3\\pong311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RAJEH\\Documents\\GitHub\\Reinforcement-Learning-Programming-Assignment-3\\pong311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RAJEH\\Documents\\GitHub\\Reinforcement-Learning-Programming-Assignment-3\\pong311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RAJEH\\Documents\\GitHub\\Reinforcement-Learning-Programming-Assignment-3\\pong311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RAJEH\\Documents\\GitHub\\Reinforcement-Learning-Programming-Assignment-3\\pong311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144\u001b[39m, in \u001b[36mReLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    141\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\RAJEH\\Documents\\GitHub\\Reinforcement-Learning-Programming-Assignment-3\\pong311\\Lib\\site-packages\\torch\\nn\\functional.py:1697\u001b[39m, in \u001b[36mrelu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   1695\u001b[39m     result = torch.relu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m   1696\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1697\u001b[39m     result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =================\n",
    "# Main training loop\n",
    "# =================\n",
    "\n",
    "env = make_env()\n",
    "stack_size = 4\n",
    "\n",
    "frame_idx = 0\n",
    "episode = 0\n",
    "\n",
    "all_rewards: list[float] = []          # score per episode\n",
    "avg_rewards_last5: list[float] = []    # average of last 5 episodes (assignment metric)\n",
    "losses: list[float] = []\n",
    "\n",
    "state_deque = None\n",
    "obs, info = env.reset()\n",
    "state, state_deque = stack_frames(state_deque, obs, stack_size)\n",
    "\n",
    "episode_reward = 0.0\n",
    "episode_steps = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "global epsilon\n",
    "epsilon = EPS_INIT\n",
    "\n",
    "while frame_idx < MAX_FRAMES and episode < MAX_EPISODES:\n",
    "    # Select action using epsilon-greedy policy\n",
    "    action = select_action(state, epsilon)\n",
    "\n",
    "    # Step in the environment\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    # Preprocess and stack next frame\n",
    "    next_state, state_deque = stack_frames(state_deque, next_obs, stack_size)\n",
    "\n",
    "    # Store transition\n",
    "    memory.push(state, action, reward, next_state, float(done))\n",
    "\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    episode_steps += 1\n",
    "    frame_idx += 1\n",
    "\n",
    "    # Update epsilon using assignment schedule\n",
    "    epsilon = update_epsilon(epsilon)\n",
    "\n",
    "    # Start learning only after replay buffer has some data\n",
    "    if len(memory) >= MIN_REPLAY_SIZE_FOR_TRAINING:\n",
    "        loss = optimize_model()\n",
    "        if loss != 0.0:\n",
    "            losses.append(loss)\n",
    "\n",
    "    # Episode end\n",
    "    if done:\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode += 1\n",
    "\n",
    "        # Average cumulative reward of last 5 episodes\n",
    "        if len(all_rewards) >= 5:\n",
    "            avg_last5 = float(np.mean(all_rewards[-5:]))\n",
    "        else:\n",
    "            avg_last5 = float(np.mean(all_rewards))\n",
    "        avg_rewards_last5.append(avg_last5)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(\n",
    "            f\"Episode {episode:4d} | Frame {frame_idx:7d} | \"\n",
    "            f\"Reward {episode_reward:6.1f} | Avg(last 5) {avg_last5:6.2f} | \"\n",
    "            f\"Epsilon {epsilon:5.3f} | Replay {len(memory):7d} | \"\n",
    "            f\"Steps {episode_steps:4d} | Elapsed {elapsed/60:5.1f} min\"\n",
    "        )\n",
    "\n",
    "        # Target network update based on EPISODES (assignment requirement)\n",
    "        if episode % TARGET_UPDATE_EPISODES == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            print(f\"--> Target network updated at episode {episode}\")\n",
    "\n",
    "        # Early stopping: only after enough frames and episodes\n",
    "        if (\n",
    "            frame_idx >= MIN_FRAMES_BEFORE_STOP and\n",
    "            len(all_rewards) >= EARLY_STOP_WINDOW\n",
    "        ):\n",
    "            recent_mean = float(np.mean(all_rewards[-EARLY_STOP_WINDOW:]))\n",
    "            if recent_mean >= EARLY_STOP_TARGET:\n",
    "                print(\n",
    "                    f\"\\nEarly stopping triggered at episode {episode}: \"\n",
    "                    f\"mean reward over last {EARLY_STOP_WINDOW} episodes = {recent_mean:.2f}\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "        # Reset for next episode\n",
    "        episode_reward = 0.0\n",
    "        episode_steps = 0\n",
    "        state_deque = None\n",
    "        obs, info = env.reset()\n",
    "        state, state_deque = stack_frames(state_deque, obs, stack_size)\n",
    "\n",
    "env.close()\n",
    "print(f\"\\nTraining finished. Episodes: {episode}, Frames: {frame_idx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d508ef",
   "metadata": {},
   "source": [
    "### 9. Training Results Visualization\n",
    "This cell plots:\n",
    "- episode rewards over time  \n",
    "- training losses  \n",
    "\n",
    "These graphs help evaluate learning progress and detect instability, divergence, or improvement in the Pong agentâ€™s performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd7b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# Plot training metrics\n",
    "# ====================\n",
    "\n",
    "if all_rewards:\n",
    "    plt.figure()\n",
    "    plt.plot(all_rewards)\n",
    "    plt.title(\"Pong DQN - Score per Episode\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.show()\n",
    "\n",
    "if avg_rewards_last5:\n",
    "    plt.figure()\n",
    "    plt.plot(avg_rewards_last5)\n",
    "    plt.title(\"Pong DQN - Average Reward of Last 5 Episodes\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Reward (last 5)\")\n",
    "    plt.show()\n",
    "\n",
    "if losses:\n",
    "    plt.figure()\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Pong DQN - Loss over Updates\")\n",
    "    plt.xlabel(\"Update step (approx.)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749538fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================\n",
    "# Evaluation helper\n",
    "# ================\n",
    "\n",
    "def evaluate_policy(num_episodes: int = 5, render: bool = False) -> None:\n",
    "    env = make_env(render_mode=\"human\" if render else None)\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        state_deque = None\n",
    "        state, state_deque = stack_frames(state_deque, obs, stack_size)\n",
    "\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "\n",
    "        while not done:\n",
    "            # Greedy policy (no epsilon)\n",
    "            with torch.no_grad():\n",
    "                state_v = torch.tensor(\n",
    "                    state,\n",
    "                    dtype=torch.float32,\n",
    "                    device=device\n",
    "                ).unsqueeze(0)\n",
    "                q_values = policy_net(state_v)\n",
    "                action = int(q_values.argmax(dim=1).item())\n",
    "\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state, state_deque = stack_frames(state_deque, next_obs, stack_size)\n",
    "\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "\n",
    "        total_reward += ep_reward\n",
    "        print(f\"Eval episode {i+1}: reward = {ep_reward:.1f}\")\n",
    "\n",
    "    avg_reward = total_reward / num_episodes\n",
    "    print(f\"Average reward over {num_episodes} eval episodes: {avg_reward:.1f}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "# Run a quick evaluation (this expects that you've trained the policy_net above)\n",
    "# You can comment this out if you only want training.\n",
    "# evaluate_policy(num_episodes=3, render=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c547ef55",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this assignment, we successfully implemented a Deep Q-Network (DQN) agent to play Pong using the Atari `PongDeterministic-v4` environment. The solution follows the core architecture described in the original DeepMind DQN paper, including:\n",
    "\n",
    "- preprocessing raw Atari frames  \n",
    "- stacking consecutive frames  \n",
    "- storing transitions in replay memory  \n",
    "- using a target network for stable learning  \n",
    "- epsilon-greedy exploration  \n",
    "- mini-batch gradient updates  \n",
    "\n",
    "During training, the agent initially performs poorly due to high exploration and lack of experience, but over time begins to learn more stable control of the paddle.\n",
    "\n",
    "The full training run requires approximately **500,000 frames**, which on this hardware corresponds to **about 4 hours** of training. With additional training (1â€“3 million frames), the agent can reach competitive Atari-level performance.\n",
    "\n",
    "This assignment demonstrates end-to-end reinforcement learning workflow, including environment setup, data preprocessing, model construction, training, and evaluation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pong311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
